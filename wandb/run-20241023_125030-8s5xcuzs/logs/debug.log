2024-10-23 12:50:30,787 INFO    MainThread:52291 [wandb_setup.py:_flush():79] Current SDK version is 0.18.5
2024-10-23 12:50:30,787 INFO    MainThread:52291 [wandb_setup.py:_flush():79] Configure stats pid to 52291
2024-10-23 12:50:30,787 INFO    MainThread:52291 [wandb_setup.py:_flush():79] Loading settings from /root/.config/wandb/settings
2024-10-23 12:50:30,787 INFO    MainThread:52291 [wandb_setup.py:_flush():79] Loading settings from /home/user/georgy/Zett/zett/wandb/settings
2024-10-23 12:50:30,788 INFO    MainThread:52291 [wandb_setup.py:_flush():79] Loading settings from environment variables: {}
2024-10-23 12:50:30,788 INFO    MainThread:52291 [wandb_setup.py:_flush():79] Applying setup settings: {'mode': None, '_disable_service': None}
2024-10-23 12:50:30,788 INFO    MainThread:52291 [wandb_setup.py:_flush():79] Inferring run settings from compute environment: {'program_relpath': 'train.py', 'program_abspath': '/home/user/georgy/Zett/zett/train.py', 'program': '/home/user/georgy/Zett/zett/train.py'}
2024-10-23 12:50:30,788 INFO    MainThread:52291 [wandb_setup.py:_flush():79] Applying login settings: {}
2024-10-23 12:50:30,788 INFO    MainThread:52291 [wandb_init.py:_log_setup():534] Logging user logs to /home/user/georgy/Zett/zett/wandb/run-20241023_125030-8s5xcuzs/logs/debug.log
2024-10-23 12:50:30,788 INFO    MainThread:52291 [wandb_init.py:_log_setup():535] Logging internal logs to /home/user/georgy/Zett/zett/wandb/run-20241023_125030-8s5xcuzs/logs/debug-internal.log
2024-10-23 12:50:30,789 INFO    MainThread:52291 [wandb_init.py:init():621] calling init triggers
2024-10-23 12:50:30,789 INFO    MainThread:52291 [wandb_init.py:init():628] wandb.init called with sweep_config: {}
config: {}
2024-10-23 12:50:30,789 INFO    MainThread:52291 [wandb_init.py:init():671] starting backend
2024-10-23 12:50:30,789 INFO    MainThread:52291 [wandb_init.py:init():675] sending inform_init request
2024-10-23 12:50:30,794 INFO    MainThread:52291 [backend.py:_multiprocessing_setup():104] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-10-23 12:50:30,795 INFO    MainThread:52291 [wandb_init.py:init():688] backend started and connected
2024-10-23 12:50:30,991 INFO    MainThread:52291 [wandb_init.py:init():783] updated telemetry
2024-10-23 12:50:31,096 INFO    MainThread:52291 [wandb_init.py:init():816] communicating run to backend with 90.0 second timeout
2024-10-23 12:50:32,294 INFO    MainThread:52291 [wandb_init.py:init():867] starting run threads in backend
2024-10-23 12:50:34,583 INFO    MainThread:52291 [wandb_run.py:_console_start():2463] atexit reg
2024-10-23 12:50:34,584 INFO    MainThread:52291 [wandb_run.py:_redirect():2311] redirect: wrap_raw
2024-10-23 12:50:34,584 INFO    MainThread:52291 [wandb_run.py:_redirect():2376] Wrapping output streams.
2024-10-23 12:50:34,584 INFO    MainThread:52291 [wandb_run.py:_redirect():2401] Redirects installed.
2024-10-23 12:50:34,592 INFO    MainThread:52291 [wandb_init.py:init():911] run started, returning control to user process
2024-10-23 12:50:35,895 INFO    MainThread:52291 [wandb_run.py:_config_callback():1390] config_cb None None {'output_dir': 'output_dir', 'init_from_params': None, 'backbone_training': 'no', 'resume_from_checkpoint': None, 'resume_from_checkpoint_reset_steps': False, 'save_state': True, 'loss': 'clm', 'mix_languages': False, 'use_adafactor': False, 'train_batch_size': 16, 'eval_batch_size': 16, 'learning_rate': [0.0003, 6e-05], 'learning_rate_alpha': 0.1, 'random_learning_rate': None, 'max_grad_norm': 0.1, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_epsilon': 1e-08, 'steps': 200, 'random_warmup_steps': 0, 'identity_steps': 10, 'warmup_steps': [10, 20], 'logging_steps': 500, 'save_steps': 10000, 'eval_steps': 300, 'eval_at_step_zero': False, 'do_train': True, 'seed': 42, 'gradient_accumulation_steps': 1, 'debug': False, 'run_backbone_in_training_mode': False, 'learnable_bias': False, 'lexical_loss_weight': 0.5, 'lexical_loss_kind': 'mse', 'apply_lexical_loss_to_init': False, 'add_target_priors_to_bias': False, 'reinit_projectors': False, 'do_cost_analysis': False}
2024-10-23 12:50:35,981 INFO    MainThread:52291 [wandb_run.py:_config_callback():1390] config_cb None None {'model_name_or_path': 'meta-llama/Meta-Llama-3-8B', 'tokenizer_name': None, 'revision': 'refs/pr/129', 'config_name': None, 'dtype': 'bfloat16'}
2024-10-23 12:50:35,983 INFO    MainThread:52291 [wandb_run.py:_config_callback():1390] config_cb None None {'train_directory': './data/train', 'valid_directory': './data/valid', 'langs': ['ru_en'], 'use_passthrough_hypernet': False, 'do_sequence_packing': True, 'add_prefix_space': True, 'n_pools': 1, 'language_sampling_alpha': 0.3, 'block_size': 128, 'extra_valid_tokenizer_names': [], 'extra_valid_files': ['./data/valid/ru_en.parquet'], 'extra_lang_codes': ['ru_en'], 'n_valid_subsample': 4000, 'target_tokenizer_name': None, 'pad_to_multiple_of': 128, 'do_tokenizer_sampling': True, 'tokenizer_sample_reweigh_temperature': inf, 'tokenizer_batch_size': 2048, 'tokenizer_sample_min': 16384, 'tokenizer_sample_max': 32768, 'tokenizer_sample_mean': 32768, 'tokenizer_sample_std': 0, 'tokenizer_noise_std': 4, 'tokenizer_noise_mean': 1e-05, 'dataloader_num_workers': 64, 'identity_n_subsample': 16384, 'n_token_subsample': None, 'sample_text_span': True, 'subsample_mode': 'random', 'hn_surface_maxlen': 7}
2024-10-23 12:50:35,985 INFO    MainThread:52291 [wandb_run.py:_config_callback():1390] config_cb None None {'hn_model_name_or_path': 'roberta-base', 'hn_surface_maxlen': 7, 'hn_n_layers': 3, 'n_embd': 4096, 'hn_hidden_size': 4096, 'hn_intermediate_size': 8192, 'hn_rescale_embeddings': True, 'use_unigram_bias': True, 'hn_embed_target_priors': False, 'hn_add_inter_token_attention': False, 'hn_inter_token_attention_bias_by_priors': True, 'hn_inter_token_attention_bias_scaler': 1.0, 'hn_n_inter_token_blocks': 16, 'hn_language_adapter_bottleneck_dim': 0, 'hn_embed_using_source_embeddings': True, 'hn_concat_last_hidden_state': False, 'hn_single_head': False, 'hn_predict_bias': True, 'hn_num_attention_heads': 16, 'hn_embed_lang_id': False, 'hn_model_type': 'roberta', 'n_langs': 1}
2024-10-23 12:57:00,821 WARNING MsgRouterThr:52291 [router.py:message_loop():77] message_loop has been closed
